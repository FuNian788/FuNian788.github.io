<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://FuNian788.github.io</id>
    <title>Zexian Li</title>
    <updated>2021-06-17T11:48:22.880Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://FuNian788.github.io"/>
    <link rel="self" href="https://FuNian788.github.io/atom.xml"/>
    <subtitle>Colorful life.</subtitle>
    <logo>https://FuNian788.github.io/images/avatar.png</logo>
    <icon>https://FuNian788.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Zexian Li</rights>
    <entry>
        <title type="html"><![CDATA[tmux配置及常用内容]]></title>
        <id>https://FuNian788.github.io/post/tmux/</id>
        <link href="https://FuNian788.github.io/post/tmux/">
        </link>
        <updated>2021-05-21T04:23:38.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li>
<ul>
<li>
<ul>
<li><a href="#%E5%8F%82%E8%80%83~tmuxconf">参考~/.tmux.conf</a></li>
<li><a href="#%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4">常用指令</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</p>
<h3 id="参考~tmuxconf">参考~/.tmux.conf</h3>
<pre><code class="language-Shell"># Status bar
# colors
set -g status-bg black
set -g status-fg white
# alignment
set-option -g status-justify centre
# spot at left
set-option -g status-left '#[bg=black,fg=yellow][#[fg=white]#S#[fg=yellow]]'
set-option -g status-left-length 20
# window list
setw -g automatic-rename on
set-window-option -g window-status-format '#[yellow]#I:#[default]#W#[fg=grey,yellow]'
set-window-option -g window-status-current-format '#[fg=white,bold]#I#[fg=yellow]:#[fg=white]#W#[fg=yellow]'
# spot at right
set -g status-right '#[fg=yellow][#[fg=white]%Y-%m-%d#[fg=yellow]]'
setw -g mouse
bind c new-window -c &quot;#{pane_current_path}&quot;

bind '&quot;' split-window -v -c &quot;#{pane_current_path}&quot;
bind % split-window -h -c &quot;#{pane_current_path}&quot;

set-window-option -g mode-keys vi
</code></pre>
<h3 id="常用指令">常用指令</h3>
<p>tmux ls<br>
tmux new -s new_name<br>
tmux kill-session -t 1</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[macOS杂记]]></title>
        <id>https://FuNian788.github.io/post/macos/</id>
        <link href="https://FuNian788.github.io/post/macos/">
        </link>
        <updated>2021-05-15T08:29:16.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li>
<ul>
<li>
<ul>
<li><a href="#%E5%86%99%E5%85%A5ntfs%E7%A1%AC%E7%9B%98%E7%9A%84%E6%96%B9%E6%B3%95">写入NTFS硬盘的方法</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</p>
<h3 id="写入ntfs硬盘的方法">写入NTFS硬盘的方法</h3>
<blockquote>
<p>感谢Luffy的<a href="https://zhuanlan.zhihu.com/p/82665550">博客</a>🚗</p>
</blockquote>
<p>Mac支持的硬盘格式是HFS+，能与Windows共用的格式是FAT32。但若硬盘在Windows下执行默认格式化(NTFS)后，Mac就只能读取其中的内容，不能写入。但其实这只是苹果和微软的版权纠纷，macOS本质上是支持NTFS读写的，只是需要一点小小的额外操作，具体解决方法记录如下：</p>
<ol>
<li>插入硬盘，在终端查看挂载方式</li>
</ol>
<pre><code class="language-Shell">mount | grep ntfs
# /dev/disk5s1 on /Volumes/Untitled (ntfs, local, nodev, nosuid, read-only, noowners)
</code></pre>
<p>其中/dev/disk5s1是设备的虚拟路径，/Volumes/Untitled是设备挂载后的实际访问路径，记住前者。</p>
<ol start="2">
<li>卸载硬盘</li>
</ol>
<p>手动卸载以read-only方式挂载的硬盘，以备后续重新挂载（不要看到卸载就慌张）</p>
<pre><code class="language-Shell">sudo umount /dev/disk5s1
</code></pre>
<ol start="3">
<li>新建挂载目录</li>
</ol>
<p>新建一个设备挂载的实际访问目录，这里以桌面上的HD文件夹为例</p>
<pre><code class="language-Shell">mkdir ~/Desktop/HD
</code></pre>
<ol start="4">
<li>以读写方式重新挂载分区</li>
</ol>
<pre><code class="language-Shell">sudo mount_ntfs -o rw, nobrowse /dev/disk5s1 ~/Desktop/HD
</code></pre>
<p>将设备虚拟路径以读写方式挂载在新建的目录下，现在即可通过桌面的HD文件夹正常访问硬盘分区。<br>
参数中的rw代表读写，nobrowse代表不在finder中显示。</p>
<ol start="5">
<li>拔出硬盘</li>
</ol>
<p>此时不能以正常方式推出硬盘，但我们可以以取消挂载的方式来实现安全推出。</p>
<pre><code class="language-Shell">sudo umount /dev/disk5s1 
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Python多进程]]></title>
        <id>https://FuNian788.github.io/post/python-duo-jin-cheng/</id>
        <link href="https://FuNian788.github.io/post/python-duo-jin-cheng/">
        </link>
        <updated>2021-05-11T08:05:16.000Z</updated>
        <content type="html"><![CDATA[<p></p>
<p>Unix/Linux下可通过<code>fork()</code>调用实现多进程。对于Python，在Unix/Linux下，其multiprocessing模块封装了<code>fork()</code>的调用，在Windows中，multiprocessing通过将父进程的所有Python对象通过pickle序列化并传到子进程中来模拟<code>fork()</code>的效果。</p>
<p>###一段最基本的多进程代码</p>
<pre><code class="language-Python">from multiprocessing import Process
import os

def run_proc(name):
    print('Run child process {} and the pid is {}.'.format(name, os.getpid()))

if __name__=='__main__':
    print('Parent process {}.'.format(os.getpid()))
    p = Process(target=run_proc, args=('test',))
    print('Child process will start.')
    p.start()   # start a child process.
    p.join()    # wait for the end of child process.
    print('Child process end.')
</code></pre>
<p>此时的输出如下所示：</p>
<pre><code class="language-Shell">Parent process 66596.
Child process will start.
Run child process test and the pid is 66599.
Child process end.
</code></pre>
<p>创建子进程，就是向Process实例传入一个执行函数和该函数的参数，使用start()方法启动该执行函数。<br>
注意，当注释掉<code>p.join()</code>语句时，主进程不会等待子进程结束，对应的输出也会发生如下的改变：</p>
<pre><code class="language-Shell">Parent process 66616.
Child process will start.
Child process end.
Run child process test and the pid is 66618.
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[einops：向量变换神器]]></title>
        <id>https://FuNian788.github.io/post/einops/</id>
        <link href="https://FuNian788.github.io/post/einops/">
        </link>
        <updated>2021-05-11T06:37:19.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li>
<ul>
<li>
<ul>
<li><a href="#reshape">RESHAPE</a></li>
<li><a href="#reduce">REDUCE</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</p>
<h3 id="reshape">RESHAPE</h3>
<pre><code class="language-Python">import einops
x = einops.rearrange(x, 'n h w c -&gt; n (h w) c')
x = einops.rearrange(x, 'n (h w) c -&gt; n c h w', h=h1)
x = einops.rearrange(x, '(n1 n2) h w c -&gt; (n1 h) (n2 w) c ', n1=2)
</code></pre>
<pre><code class="language-Python">from einops.layes.torch import Rearrange
self.net = nn.Sequential(
    nn.LayerNorm(dim),
    Rearrange('n h w -&gt; h w n')
)
</code></pre>
<h3 id="reduce">REDUCE</h3>
<pre><code class="language-Python"># str: mean, min, max, sum, prod
x = einops.reduce(x, 'n c h w -&gt; n h w', 'mean')    # average over channel

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[论文简读]]></title>
        <id>https://FuNian788.github.io/post/paper-easy-read/</id>
        <link href="https://FuNian788.github.io/post/paper-easy-read/">
        </link>
        <updated>2021-05-10T05:59:27.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li>
<ul>
<li>
<ul>
<li><a href="#1-mlp-mixer-an-all-mlp-architecture-for-vision">(1) MLP-Mixer: An all-MLP Architecture for Vision</a></li>
<li><a href="#2-acnet-strengthening-the-kernel-skeletons-for-powerful-cnn-via-asymmetric-convolution-blocks">(2) ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</p>
<h3 id="1-mlp-mixer-an-all-mlp-architecture-for-vision">(1) <a href="https://arxiv.org/pdf/2105.01601.pdf">MLP-Mixer: An all-MLP Architecture for Vision</a></h3>
<p><strong>Make MLP great again</strong>🌝</p>
<p>MLP-Mixer包含两种MLP：token-mixing MLP每次处理单个通道，进行不同patch间的信息交换(图中一个patch使用一种颜色)；channel-mixing MLP每次处理单个patch，进行通道上的信息交换。<br>
如下图所示，每个Mixer Layer包含一个token-mixing MLP(MLP1)和channel-mixing MLP(MLP2)。具体地，每个MLP包含两个fc层和一个GELU层(<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>G</mi><mi>E</mi><mi>L</mi><mi>U</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>x</mi><mi>P</mi><mo>(</mo><mi>X</mi><mo>≤</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>x</mi><mi mathvariant="normal">Φ</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">GELU(x) = xP(X\le x) = x\Phi (x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">G</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mord mathdefault">L</span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">x</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">x</span><span class="mord">Φ</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>)；token-mixing MLP共享参数，<br>
<img src="https://FuNian788.github.io/post-images/object_detection/mixer_1.png" alt="mixer结构" loading="lazy"><br>
Depthwise Separable Convolution将卷积分成depthwise convolution和pointwise convolution两步来实现，这很有那味。但Depthwise Separable Convolution对不同通道使用不同卷积核，而Mixer对所有通道均使用相同的卷积核。</p>
<p>伪代码如下所示：<br>
<img src="https://FuNian788.github.io/post-images/object_detection/mixer_2.png" alt="伪代码" loading="lazy"><br>
对于(N, H=224, W=224, C)的原图片，设patch大小为(s_height=16, s_width=16)，可得到h=14*w=14个patch。<br>
MlpMixer中，Line36相当于对(N, H, W, C)的原图片，挨个patch处理其特征，输出(N, h, w, c)的特征，Line37将其resize到(N, hw, c)。将该特征送到token-mixing MLP中时，将其reshape到(N, hw, c)后接两个通道数为c的fc，将该特征送到channel-mixing时，将其reshape到(N, c, hw)后接两个通道数为hw的fc。</p>
<h3 id="2-acnet-strengthening-the-kernel-skeletons-for-powerful-cnn-via-asymmetric-convolution-blocks">(2) <a href="https://arxiv.org/pdf/1908.03930.pdf">ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks</a></h3>
<p><strong>重参数化：轻微延长训练时间以在不涨参数量的情况下获得涨点。</strong><br>
举例，<br>
<img src="https://FuNian788.github.io/post-images/object_detection/ACNet_1.png" alt="重参数化" loading="lazy"></p>
<p>宏观上来看ACNet分为训练和推理阶段，训练阶段重点在于强化特征提取，实现效果提升。而测试阶段重点在于卷积核融合，不增强任何计算量。</p>
<p>训练阶段：因为[公式]卷积是大多数网络的基础组件，因此ACNet的实验都是针对[公式]卷积进行的。训练阶段就是将现有网络中的每一个[公式]卷积换成[公式]卷积+[公式]卷积+[公式]卷积共三个卷积层，最终将这三个卷积层的计算结果进行融合获得卷积层的输出。因为这个过程中引入的[公式]卷积和[公式]卷积是非对称的，所以将其命名为Asymmetric Convolution。<br>
推理阶段：如Figure1右图所示，这部分主要是对三个卷积核进行融合。这部分在实现过程中就是使用融合后的卷积核参数来初始化现有的网络，因此在推理阶段，网络结构和原始网络是完全一样的了，只不过网络参数采用了特征提取能力更强的参数即融合后的卷积核参数，因此在推理阶段不会增加计算量。<br>
总结一下就是ACNet在训练阶段强化了原始网络的特征提取能力，在推理阶段融合卷积核达到不增加计算量的目的。虽然训练时间增加了一些时间，但却换来了对推理无痛的精度提升，怎么看都是一笔非常划算的交易。下面的Table3展示出来，对于AlexNet提升了比较多，而对ResNet和DenseNet提升不到一个百分点，不过考虑到这个提升是白赚的也还是非常值得肯定的。</p>
<p>换取不涨参数的</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer in CV]]></title>
        <id>https://FuNian788.github.io/post/transformer-in-cv/</id>
        <link href="https://FuNian788.github.io/post/transformer-in-cv/">
        </link>
        <updated>2021-05-07T09:25:59.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li>
<ul>
<li>
<ul>
<li><a href="#1-2017-nipsattention-is-all-you-need">(1) (2017 NIPS)Attention is all you need</a></li>
<li><a href="#2-fairdetr-end-to-end-object-detection-with-transformers">(2) (FAIR)DETR: End-to-End Object Detection with Transformers</a></li>
<li><a href="#3-2021-iclr-oraldeformable-detr-deformable-transformers-for-end-to-end-object-detection">(3) (2021 ICLR oral)Deformable DETR: Deformable Transformers for End-to-End Object Detection</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</p>
<h3 id="1-2017-nipsattention-is-all-you-need">(1) <a href="https://arxiv.org/abs/1706.03762">(2017 NIPS)Attention is all you need</a></h3>
<figure data-type="image" tabindex="1"><img src="https://FuNian788.github.io/post-images/object_detection/transformer_1.png" alt="transformer结构图" loading="lazy"></figure>
<h3 id="2-fairdetr-end-to-end-object-detection-with-transformers">(2) <a href="https://arxiv.org/abs/2005.12872">(FAIR)DETR: End-to-End Object Detection with Transformers</a></h3>
<ul>
<li>Motivation</li>
</ul>
<p>提出了一个基于transformer的目标检测结构，大大简化了检测流程(移除了很多人为设计的模块eg NMS、基于先验知识的方法eg anchor生成)。</p>
<ul>
<li>主要贡献</li>
</ul>
<p>端到端训练、单次并行测试：把目标检测任务视为一个集合预测问题。输入可训练的object query，获取预测框，再使用匈牙利匹配算法对GT和predicted的二分图进行匹配，从而计算损失函数。</p>
<figure data-type="image" tabindex="2"><img src="https://FuNian788.github.io/post-images/object_detection/DETR_1.png" alt="DETR结构图" loading="lazy"></figure>
<ul>
<li>
<p>overall</p>
</li>
<li>
<p>实现细节</p>
</li>
</ul>
<p>匈牙利算法：<br>
参考Pecco的一篇<a href="https://zhuanlan.zhihu.com/p/96229700">知乎博客</a>，可将匈牙利算法的核心代码展示如下：</p>
<pre><code class="language-C">int M, N;            //M, N分别表示左、右侧集合的元素数量
int Map[MAXM][MAXN]; //邻接矩阵存图
int p[MAXN];         //记录当前右侧元素所对应的左侧元素
bool vis[MAXN];      //记录右侧元素是否已被访问过
bool match(int i)
{
    for (int j = 1; j &lt;= N; ++j)
        if (Map[i][j] &amp;&amp; !vis[j]) //有边且未访问
        {
            vis[j] = true;                 //记录状态为访问过
            if (p[j] == 0 || match(p[j])) //如果暂无匹配，或者原来匹配的左侧元素可以找到新的匹配
            {
                p[j] = i;    //当前左侧元素成为当前右侧元素的新匹配
                return true; //返回匹配成功
            }
        }
    return false; //循环结束，仍未找到匹配，返回匹配失败
}
int Hungarian()
{
    int cnt = 0;
    for (int i = 1; i &lt;= M; ++i)
    {
        memset(vis, 0, sizeof(vis)); //重置vis数组
        if (match(i))
            cnt++;
    }
    return cnt;
}
</code></pre>
<ul>
<li>改进/Challenge/idea/Que</li>
</ul>
<h3 id="3-2021-iclr-oraldeformable-detr-deformable-transformers-for-end-to-end-object-detection">(3) <a href="https://arxiv.org/abs/2010.04159">(2021 ICLR oral)Deformable DETR: Deformable Transformers for End-to-End Object Detection</a></h3>
<ul>
<li>
<p>前导知识<br>
ICCV 2017 Deformable Convolutional Networks提出了Deformable conv，如下图所示，deformable conv为基本卷积时围绕中心规整排列的所有采样点均叠加一个位置offset，在移动后的位置进行信息采样，能有效捕获物体的关键信息。<br>
<img src="https://FuNian788.github.io/post-images/object_detection/deform_conv_1.png" alt="deformable conv" loading="lazy"><br>
多层下基本卷积和deformable conv的区别示意图：<br>
<img src="https://FuNian788.github.io/post-images/object_detection/deform_conv_2.png" alt="deformable conv示意图" loading="lazy"><br>
该方法很容易理解，但代码实现时需要考虑：<br>
Q1 如何将deformable conv变成单独的一个层？<br>
A1 不对卷积核进行迁移，而是重新整合feature map，再进行正常的卷积。<br>
Q2 如何有效实现反向传播？<br>
feature map整合时，由于存在像素级偏移操作，而偏移量可能为浮点数类型。</p>
</li>
<li>
<p>Motivation<br>
DETR训练收敛速度慢：注意力模块在初始化时，对feature map所有位置给了近乎相同的权重，收敛就会很慢；</p>
</li>
</ul>
<p>，在小物体检测效果不佳。前者是因为</p>
<p>DETR受Transformer Attention module的限制，其收敛速度较慢，特征的分辨率也受限。</p>
<p>文章是detr的改进，主要解决detr收敛慢的问题，此外由于有限的特征空间分辨率，在检测小物体方面的性能相对较低<br>
作者将原始detr的encoder部分换成deformable attention module，感觉这一点很重要，此外引入了多尺度的特征，来提升小物体的检出性能。</p>
<p>较慢的收敛速度和</p>
<ul>
<li>
<p>主要贡献</p>
</li>
<li>
<p>overall</p>
</li>
<li>
<p>实现细节</p>
</li>
<li>
<p>改进/Challenge/idea/Que</p>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[目标检测论文阅读]]></title>
        <id>https://FuNian788.github.io/post/detection/</id>
        <link href="https://FuNian788.github.io/post/detection/">
        </link>
        <updated>2021-04-26T03:23:13.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0">方法综述</a></li>
<li><a href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87">评价指标</a></li>
<li><a href="#%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86">常用数据集</a></li>
<li><a href="#%E6%8C%87%E6%A0%87%E6%AF%94%E5%AF%B9">指标比对</a></li>
<li><a href="#%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0">论文笔记</a>
<ul>
<li><a href="#1-2021-cvpryolof-you-only-look-one-level-feature">(1) (2021 CVPR)YOLOF: You Only Look One-level Feature</a></li>
<li><a href="#2-fairretinanet-focal-loss-for-dense-object-detection">(2) (FAIR)RetinaNet: Focal Loss for Dense Object Detection</a></li>
<li><a href="#3-2019-iccvfcos-fully-convolutional-one-stage-object-detection">(3) (2019 ICCV)FCOS: Fully Convolutional One-Stage Object Detection</a></li>
<li><a href="#4-2020-cvpr-oralatss-bridging-the-gap-between-anchor-based-and-anchor-free-detection-via-adaptive-training-sample-selection">(4) (2020 CVPR oral)ATSS: Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</a></li>
<li><a href="#5-2019-cvprhr-net-deep-high-resolution-representation-learning-for-visual-recognition">(5) (2019 CVPR)HR-Net: Deep High-Resolution Representation Learning for Visual Recognition</a></li>
</ul>
</li>
<li><a href="#%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B">基础模型</a>
<ul>
<li><a href="#1-2017-cvprresnext-aggregated-residual-transformations-for-deep-neural-networks">(1) (2017 CVPR)ResNeXt: Aggregated Residual Transformations for Deep Neural Networks</a></li>
<li><a href="#n-2021-cvprpaper-name">(n) (2021 CVPR)PAPER NAME</a></li>
</ul>
</li>
<li><a href="#%E5%9F%BA%E6%9C%AC%E5%B8%B8%E8%AF%86">基本常识</a></li>
</ul>
</li>
</ul>
</p>
<h2 id="方法综述">方法综述</h2>
<p>目标检测的主流方法是anchor-based，也存在一些anchor-free的尝试。<br>
<strong>anchor-based</strong>：在图片上铺海量的预设anchor，随后对anchor进行种类预测和n次边界回归。通常来说，双阶段方法对边界修正的次数会多于单阶段方法，所以二阶段方法通常有较高的精度，单阶段方法常有较高的计算效率。</p>
<ul>
<li>one-stage<br>
均匀且密集地采样以获得海量候选框，随后进行分类(eg SSD)</li>
<li>two-stage<br>
在筛选过的、稀疏的候选框(滤掉了绝大多数背景/负样本)上进行分类(eg Faster R-CNN)</li>
</ul>
<p><strong>anchor-free</strong>：不使用预设anchor，直接检测物体。</p>
<ul>
<li>keypoint-based<br>
先定位一些预设的/自学习得到的关键点，再依此生成候选框以检测物体(eg Cornernet预测候选框的左上角和右下角点，再进行组合)</li>
<li>center-based<br>
将物体正中心点/中心区域视为正样本，再基于此预测该位置到目标四条边框的距离(eg FCOS将前景框的所有像素点均视为正样本，随后对每个点回归其到框边界的距离)</li>
</ul>
<h2 id="评价指标">评价指标</h2>
<p>mAP</p>
<h2 id="常用数据集">常用数据集</h2>
<p>MS COCO：包含80类物体。数据集划分如下：train有80k图片，val有40k图片。<br>
MSCOCO 2017： training118k validation5k(val) testing about20k无标注(test-dev)<br>
trainval35k部分的115K图片(train的80k图片+)用于训练，minival部分的5k图片用于validation。</p>
<h2 id="指标比对">指标比对</h2>
<p>以COCO数据集AP作为基准指标，比较各方法如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">ID</th>
<th style="text-align:center">paper</th>
<th style="text-align:center">AP</th>
<th style="text-align:center">AP50</th>
<th style="text-align:center">AP75</th>
<th style="text-align:center">APs</th>
<th style="text-align:center">APm</th>
<th style="text-align:center">APl</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">YOLOF</td>
<td style="text-align:center">37.7</td>
<td style="text-align:center">56.9</td>
<td style="text-align:center">40.6</td>
<td style="text-align:center">19.1</td>
<td style="text-align:center">42.5</td>
<td style="text-align:center">53.2</td>
</tr>
</tbody>
</table>
<h2 id="论文笔记">论文笔记</h2>
<h3 id="1-2021-cvpryolof-you-only-look-one-level-feature">(1) <a href="https://arxiv.org/abs/2103.09460">(2021 CVPR)YOLOF: You Only Look One-level Feature</a></h3>
<ul>
<li>Motivation</li>
</ul>
<p>对RetinaNet的FPN结构做消解实验发现：SiMo仅轻微掉点，故C5特征可能就已涵盖了检测所需要的足够信息；SiMo明显优于MiSo，故分治策略(即多检测头)的重要性远大于融合输入特征的重要性。由上述两点，决定将输入从多输入砍到单输入。<br>
通常认为FPN的核心优势在于多特征融合(multi-scale feature fusion)和分治策略(divide-and-conquer)。本文提出，FPN最大的优势是在密集目标检测时使用分治策略(逐级检测)来进行优化。<br>
<img src="https://FuNian788.github.io/post-images/object_detection/yolof_1.png" alt="RetinaNet-FPN-单多输入-单多输出" loading="lazy"></p>
<p>分治策略可以视为一种优化的方法：将大问题拆分成小问题来解决。但其多头结构降低检测速度，使结构更复杂，带来了更大的空间负担。决定将多输出砍到单输出。</p>
<ul>
<li>主要贡献</li>
</ul>
<p>将FPN简化成单输入(32倍下采样的C5特征)单输出的结构，同时为了弥补SiSo到MiMo的性能差距：</p>
<ol>
<li>
<p>C5特征的感受野大小受限<br>
提出了空洞编码器(Dilated Encoder)来获取多尺度特征</p>
</li>
<li>
<p>Positive anchor对应的GT大小不均衡<br>
提出均衡匹配策略(Uniform Matching)来解决单特征图中稀疏anchor引起的positive anchor不平衡问题。</p>
</li>
</ol>
<p>COCO数据集上，在效果相当的同时，YOLOF可以：<br>
比FPN版本速度快了2.5倍；比DETR的训练轮次少了7倍；比YOLOv4快了13%。</p>
<ul>
<li>
<p>overall<br>
<img src="https://FuNian788.github.io/post-images/object_detection/yolof_2.png" alt="YOLOF结构" loading="lazy"></p>
</li>
<li>
<p>实现细节</p>
</li>
</ul>
<ol>
<li>
<p>空洞编码器<br>
C5特征的感受野很小，只能cover小物体；使用连续的dilated conv疯狂扩展感受野后，只能捕获大物体。故使用残差结构+空洞卷积，使模型既可以捕获小物体，也能捕获大物体。<br>
<img src="https://FuNian788.github.io/post-images/object_detection/yolof_3.png" alt="dilated encoder" loading="lazy"><br>
模型的Projector部分，使用1*1卷积减少通道数，使用3*3卷积修复语义信息；残差块部分使用四个不同dilated rate，不共享参数的block，每个block中，第一个conv降通道数，第二个dilated conv扩大感受野，第三个conv恢复通道数。</p>
</li>
<li>
<p>均衡匹配策略<br>
MiMo模型可以在不同的输出层级上定义不同大小的anchor，然而SiSo模型只有单层输出，对应的anchor数量自然变少。anchor本就容易和大尺度样本有较大的overlap，假设将与任GT bbox的IoU&gt;0.5的anchor定义为positive anchor，那单输入特征图&amp;少anchor时，anchor的不均衡就更为明显：positive anchor主要由大样本占据；模型的重心在大样本上，检出小样本的数量变少。<br>
<img src="https://FuNian788.github.io/post-images/object_detection/yolof_4.png" alt="positive anchor均衡性" loading="lazy"><br>
Uniform Matching：对于每个GT bbox，选取距其最近的K=4个anchor作为positive anchor。<br>
YOLOF在C5 feature的每个位置上构建5个anchor，尺度分别为{32, 64, 128, 256, 512}。</p>
</li>
</ol>
<ul>
<li>改进/Challenge/idea/Que</li>
</ul>
<hr>
<h3 id="2-fairretinanet-focal-loss-for-dense-object-detection">(2) <a href="https://arxiv.org/abs/1708.02002">(FAIR)RetinaNet: Focal Loss for Dense Object Detection</a></h3>
<ul>
<li>Motication<br>
one-stage方法和two-stage方法的核心区别在于：two-stage送去分类器的候选框是稀疏的(过滤了绝大多数背景样本)，而为了实现检测任务，one-stage必须在图片内进行密集的均匀采样，得到未经过滤的候选框并依此去cover所有空间位置。自然地，one-stage这种采样方法得到的候选框中以容易被分类的背景框居多。two-stage方法的候选框大概在1~2k个，one-stage方法的候选框却可达到100k个左右。</li>
</ul>
<p>为什么two-stage方法常常更准呢？本文认为其核心问题在于<strong>训练检测器时样本所属前景-背景类别的不均衡性</strong>(the extreme foreground-background class imbalance encountered during training of dense detectors)。基于此，作者在交叉熵损失函数的基础上提出了Focal Loss，有效阻止海量的、易被分类的背景样本主导训练过程，在对其进行降权的同时，让网络更加关注难样本(更多是正样本)的分类过程。总体来说，当网络对一个样本的预测越准确，该样本对loss的贡献程度越低。</p>
<ul>
<li>
<p>主要贡献<br>
提出Focal Loss来解决正负样本不均衡的问题；基于此提出one-stage的RetinaNet，在保持优良速度的同时具有大幅超过two-stage方法的精度，达到SOTA。</p>
</li>
<li>
<p>overall<br>
对于交叉熵损失函数，在真值类别时，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mo>−</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mi>P</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">Loss = -log(P)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">−</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mclose">)</span></span></span></span>，在其他类别时，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mo>−</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mn>1</mn><mo>−</mo><mi>P</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">Loss = -log(1 - P)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">−</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mclose">)</span></span></span></span>。作者认为在此中，简单样本带来的loss不足够小，当简单的背景样本过于多的时候还是可以主宰整个训练过程。为了将训练的注意力集中在难样本上，作者提出了如下图的focal loss。<br>
<img src="https://FuNian788.github.io/post-images/object_detection/retinanet_1.png" alt="positive anchor均衡性" loading="lazy"><br>
假设模型输出概率<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span></span></span></span>。对应地，在两种情况下分析模型处理简单样本的过程：在真值类别下，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span></span></span></span>很大，接近于1，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mi>t</mi></msub><mo>=</mo><mi>P</mi></mrow><annotation encoding="application/x-tex">P_{t} = P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span></span></span></span>，这时损失函数的值就超小；在其他类别下，预测得很准时<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span></span></span></span>应该很小，接近于0，此时的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mi>t</mi></msub><mo>=</mo><mn>1</mn><mo>−</mo><mi>P</mi></mrow><annotation encoding="application/x-tex">P_{t} = 1 - P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span></span></span></span>还是很大，损失函数仍然很小。反之，处理难样本时的损失函数没有被削减。</p>
</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://FuNian788.github.io/post-images/object_detection/retinanet_2.png" alt="RetinaNet结构图" loading="lazy"></figure>
<ul>
<li>实现细节</li>
</ul>
<ol>
<li>anchor<br>
在每个位置上，</li>
</ol>
<p>在不同的输出特征图上限制检出物体的大小。</p>
<ul>
<li>改进/Challenge/idea/Que</li>
</ul>
<hr>
<h3 id="3-2019-iccvfcos-fully-convolutional-one-stage-object-detection">(3) <a href="https://arxiv.org/abs/1904.01355">(2019 ICCV)FCOS: Fully Convolutional One-Stage Object Detection</a></h3>
<ul>
<li>针对痛点：</li>
</ul>
<ol>
<li>anchor-based方法需要人为设计框的尺寸及超参数，这些参数的优劣会造成显著地性能差异，且人为设计的候选框很难匹配尺度多变的目标；</li>
<li>anchor-based方法常需要海量的候选框(eg FPN在800见方的图片中需要180K个anchor)，过多的负样本框会造成训练时的样本不平衡；训练时对IoU的不断计算也会导致较大的计算开销；</li>
<li>传统的anchor-free &amp; one-stage方法有<strong>两大弊端</strong>，一是只将anchor中心点所在的网格试做正样本，这种样本不平衡对recall有较大负面影响；一是很难处理重叠物体(anchor方法可很好的解决重叠问题)。</li>
</ol>
<ul>
<li>主要贡献：</li>
</ul>
<ol>
<li>FCN(fully convolutional networks)已其他领域开展得如火如荼，eg语义分割，首在object detection领域基于FCN方法实现的FCOS有助于复用其他领域经验。</li>
<li>anchor-free的FCOS省去了调参的负担，在计算更轻量、训练更容易的同时有着不亚于two-stage/anchor-based方法的精度，启发了对anchor必要性的思考。</li>
<li>提出one-stage的FCN-based网络，提出基于FPN的多尺度预测和center-ness方法，有效解决了当下存在的两大弊端。</li>
</ol>
<ul>
<li>实现流程：<br>
网络的流程图如下所示：<br>
<img src="https://FuNian788.github.io/post-images/TAD/FCOS_1.png" alt="流程图" loading="lazy"><br>
对于单个候选框，每层网络输出一个K维分类label、一个4维距离坐标和一个center-ness得分。该网络较常用的anchor-based方法减少了近9倍的参数量。</li>
<li>实现细节：</li>
</ul>
<ol>
<li>one-stage FCN网络<br>
针对弊端一，FCOS不再只将中心点所在的网格视为正样本，而是将GT bbox所覆盖的所有网格均视为正样本。因此，FCOS舍弃了anchor-based方法中先确定候选框中心位置，再以依此对anchor进行回归的做法，具体地，如下左图所示，对每个前景框的每个点(location)，都预测其到GT bbox的上下左右四条边的距离[t, b, l, r]，该做法与FCN-based方法在语义分割领域的实现思路一致，且值得注意的是，与anchor-based方法仅将与GT bbox有较高IoU的anchor作为正样本相比，FCOS将GT内的所有像素均作为正样本进行训练，无形中获取了更多更准的信息。<br>
<img src="https://FuNian788.github.io/post-images/TAD/FCOS_2.png" alt="示例图" loading="lazy"><br>
对于某层feature map上的location点(x, y)，基于当前层的步长s，先将其映射到原始输入图上的点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mo>⌊</mo><mfrac><mi>s</mi><mn>2</mn></mfrac><mo>⌋</mo><mo>+</mo><mi>x</mi><mi>s</mi><mo separator="true">,</mo><mo>⌊</mo><mfrac><mi>s</mi><mn>2</mn></mfrac><mo>⌋</mo><mo>+</mo><mi>y</mi><mi>s</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(\lfloor \frac s2 \rfloor + xs, \lfloor \frac s2 \rfloor + ys)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.095em;vertical-align:-0.345em;"></span><span class="mopen">(</span><span class="mopen">⌊</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">⌋</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.095em;vertical-align:-0.345em;"></span><span class="mord mathdefault">x</span><span class="mord mathdefault">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mopen">⌊</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">⌋</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord mathdefault">s</span><span class="mclose">)</span></span></span></span>，用映射后的坐标再进行到框边距离的预测，使用指数函数输出四个非负距离量，考虑到不同尺度feature map预测物体大小不同，单纯的指数函数得到距离有失偏颇，FCOS在指数函数中加入可学习参数s(distance = exp(s * x))来适应各个尺度的预测。<br>
损失函数方面，分类方面使用focal loss，坐标回归方面使用IOU loss。</li>
<li>基于FPN的多尺度预测<br>
为解决重叠候选框时的分类问题(如上图右)，anchor-based方法在不同特征尺度设计不同尺寸的候选框，FCOS则遵循FPN的思路，在不同的feature map层检测不同尺度的目标，且<strong>通过阈值直接限制了每一层检出框的尺寸大小</strong>。在FCOS的五个回归branch中，若预测得到的边长长度不符合当前层的长度限制，直接将其视为负样本，这样便粗暴地解决了不同层内目标含有重叠区域的问题。对于同一层内的重叠区域，FCOS直接使用最小的区域作为回归目标(即使同一层内两个同类物体有重叠，小物体会马上被检出，而大物体也定会在未重叠的区域被检出；但不得不说该方法存在理论缺陷，就是同一层内两个不同类物体有重叠，在重叠部分的location可能会返回A物体的种类和B物体的候选框)。依此，FCOS在多尺度预测的同时很好地解决了物体重叠的问题。<br>
如流程图所示，C3、C4、C5是backbone的feature map，通过1*1的卷积层得到P3、P4、P5，而P6和P7则是由P5、P6经过stride为2的卷积层得到的，不同层之间的heads共享权重。<br>
直觉上可能会认为FCOS的BPR(best possible recall，检测器能实现的recall上限，只要有一个anchor涉及到GT框便将其纳入BPR计算范畴内)会受到FCN-based方法中大步长的制约，但实际上FCOS的BPR甚至优于传统的anchor-based方法，所以recall不是FCOS需要着重解决的问题，或者说FPN解决了这一问题。</li>
<li>center-ness分支<br>
在前两步之后，FCOS的性能较anchor-based方法仍有一定的gap，这主要是由一些距物体中心较远的location产生的低质量候选框导致的，FCOS在不引入更多超参数的情况下提出了center-ness分支直接而有效地减少了低质量候选框。<br>
<img src="https://FuNian788.github.io/post-images/TAD/FCOS_3.png" alt="center-ness" loading="lazy"><br>
center-ness的核心思想是，一个GT bbox内的点很多，但它们对目标的贡献是不同的。偏图像中心的点包含了更多的目标信息，理应得到重视；偏图像边界的点包含的信息相对较少，甚至点可能就不在目标上，这些location提出的候选框的质量常较低，理应设置更小的权重。center-ness表征了一个点到其预测候选框中心的距离，其表达式如下所示：</li>
</ol>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>r</mi><mo>−</mo><mi>n</mi><mi>e</mi><mi>s</mi><mi>s</mi><mo>=</mo><msqrt><mrow><mfrac><mrow><mi>m</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>l</mi><mo separator="true">,</mo><mi>r</mi><mo>)</mo></mrow><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mi>l</mi><mo separator="true">,</mo><mi>r</mi><mo>)</mo></mrow></mfrac><mo>∗</mo><mfrac><mrow><mi>m</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>t</mi><mo separator="true">,</mo><mi>b</mi><mo>)</mo></mrow><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mi>t</mi><mo separator="true">,</mo><mi>b</mi><mo>)</mo></mrow></mfrac></mrow></msqrt></mrow><annotation encoding="application/x-tex">center-ness = \sqrt{ \frac {min(l, r)}{max(l, r)} *  \frac {min(t, b)}{max(t, b)} }
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69841em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">c</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span><span class="mord mathdefault">e</span><span class="mord mathdefault">s</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.04em;vertical-align:-1.160625em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.879375em;"><span class="svg-align" style="top:-5em;"><span class="pstrut" style="height:5em;"></span><span class="mord" style="padding-left:1em;"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">m</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">b</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">m</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">b</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-3.839375em;"><span class="pstrut" style="height:5em;"></span><span class="hide-tail" style="min-width:1.02em;height:3.08em;"><svg width='400em' height='3.08em' viewBox='0 0 400000 3240' preserveAspectRatio='xMinYMin slice'><path d='M473,2793c339.3,-1799.3,509.3,-2700,510,-2702
c3.3,-7.3,9.3,-11,18,-11H400000v40H1017.7s-90.5,478,-276.2,1466c-185.7,988,
-279.5,1483,-281.5,1485c-2,6,-10,9,-24,9c-8,0,-12,-0.7,-12,-2c0,-1.3,-5.3,-32,
-16,-92c-50.7,-293.3,-119.7,-693.3,-207,-1200c0,-1.3,-5.3,8.7,-16,30c-10.7,
21.3,-21.3,42.7,-32,64s-16,33,-16,33s-26,-26,-26,-26s76,-153,76,-153s77,-151,
77,-151c0.7,0.7,35.7,202,105,604c67.3,400.7,102,602.7,104,606z
M1001 80H400000v40H1017z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.160625em;"><span></span></span></span></span></span></span></span></span></span></p>
<p>当该点为候选框中心时，center-ness的值为1，该点离中心越远，其值越接近0，使用交叉熵损失函数进行训练。center-ness仅在测试时使用，将其与分类得分(classification score)相乘，共同作为候选框的得分，随后进行NMS筛选。</p>
<ul>
<li>对动作检测的启发</li>
</ul>
<ol>
<li>FCN网络结构</li>
<li>动作框预测方式：遍历视频的每一个时刻，返回其到所属动作开始/结束时刻的时间距离</li>
<li>基于FPN的多尺度预测：如果出现了动作套娃(eg运动里套打篮球)，可以考虑使用这种方法来一起检出；其他情况下则不合适，eg峰谷峰时如何只取两个峰。但多尺度思路可以应用于尺度不同的任务。</li>
<li>如果一个候选框和GT的IoU较低，其就是一个低质量候选框，这时如果其confidence得分很高，就是Fasle Positive了。center-ness的应用，可以有效解决当前实践中<strong>置信度最高的候选框可能不是最好的</strong>这一问题。<br>
FCOS认为中心lcoation预测的框是高质量的，边缘location预测的框是低质量的，通过表征location位置的center-ness指标来衡量框的质量，利用其削减低质量候选框得分，从而在NMS阶段保留下真正好的候选框。<br>
在动作检测中，center-ness可以IoU的形式出现，拓展地，可以参考IoUNet新建一个独立的网络去预测候选框和GT的IoU以参与得分评价(FCOS的center-ness计算更为简洁，只是在原有预测基础上进行简单比例计算，并没有涉及神经网络架构)；而如果搭建FCOS-based的动作检测框架，可以直接利用center-ness思路，但要考虑动作并没有明确中心点这一问题。</li>
</ol>
<hr>
<h3 id="4-2020-cvpr-oralatss-bridging-the-gap-between-anchor-based-and-anchor-free-detection-via-adaptive-training-sample-selection">(4) <a href="https://arxiv.org/abs/1912.02424">(2020 CVPR oral)ATSS: Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</a></h3>
<ul>
<li>Motivation<br>
对比anchor-based one-stage RetinaNet和anchor-free center-based FCOS，二者有以下三个明显的区别：</li>
</ul>
<ol>
<li>每个像素点(location)处铺设anchor的数量<br>
RetinaNet以每个location为中心位置铺设多个anchor；FCOS视每个location为一个点，再回归该点到该点所在候选框的边界的距离。</li>
<li><strong>定义正负训练样本的方法</strong><br>
RetinaNet将与GT bbox的IoU大于阈值的anchor视为正样本；FCOS将GT bbox内的所有像素点视为正样本。</li>
<li>在何种载体上回归<br>
RetinaNet在铺设的anchor上回归；FCOS在像素点上进行物体定位。<br>
实验表明，性能gap的核心在于第二点。</li>
</ol>
<ul>
<li>主要贡献<br>
首先指出：anchor-based方法和anchor-free方法的本质区别在于<strong>如何定义正负训练样本</strong>，也正是这带来了性能上的差距。如果在训练中使用相同的定义正负样本的方法，那无论是基于box还是point进行回归，最后都不会有显著的性能差异。</li>
</ul>
<p>提出了一种自适应的训练样本筛选策略(ATSS, adaptive training sample selection)来依据目标的统计特征自动地筛选样本。</p>
<ul>
<li>实验过程</li>
</ul>
<ol>
<li>
<p>补齐性能差距<br>
对于mAP，RetinaNet为32.5，FCOS为37.8。为探究anchor-based和anchor-free方法的真正区别，需消除FCOS各种trick对于性能gap的影响：RetinaNet每个像素点处仅许产生一个候选框，再将FCOS的各种trick添加到RetinaNet上(In GT box代表限制GT的正样本数量)：<br>
<img src="https://FuNian788.github.io/post-images/object_detection/ATSS_1.png" alt="RetinaNet FCOS实验结果对比" loading="lazy"><br>
这时的性能：RetinaNet 37.0 vs FCOS 37.8，仍存在0.8个点的差距。此时anchor-based和anchor-free方法仅有两点不同：检测器分类头对于正负样本的定义、检测器回归头是从anchor回归还是从中心点回归。下面对这两点进行进一步实验，探寻到底哪一个才是问题的关键。</p>
</li>
<li>
<p>分类part<br>
RetinaNet在不同的FPN层级使用IoU来定义样本：将每个目标对应的best anchor和IoU大于阈值的anchor标为正样本，将IoU小于阈值的anchor标为负样本，舍弃剩余anchor。<br>
FCOS在不同的FPN层级基于空间位置和目标尺度来定义样本：中心点在GT bbox内的为候选正样本，再基于目标尺度和FPN层级的匹配性从中筛选得到真正的正样本，剩余为负样本。<br>
<img src="https://FuNian788.github.io/post-images/object_detection/ATSS_2.png" alt="RetinaNet FCOS定义正负样本方法" loading="lazy"></p>
</li>
<li>
<p>回归part<br>
RetinaNet回归4个offset(x, y, w, h)，FCOS回归到边界的四个距离(l, r, t, b)。<br>
<img src="https://FuNian788.github.io/post-images/object_detection/ATSS_4.png" alt="RetinaNet FCOS回归方法对比" loading="lazy"></p>
</li>
<li>
<p>性能对比<br>
实验中，只改变了定义样本的方式，但数据可以从多个角度进行分析：<br>
看每一列，回归的方法确定后，RetinaNet和FCOS均会因定义样本的方式而产生较大的性能gap。<br>
<img src="https://FuNian788.github.io/post-images/object_detection/ATSS_3.png" alt="RetinaNet FCOS性能对比" loading="lazy"><br>
以上实验说明，anchor-based和anchor-free方法性能差距的核心在于<strong>训练时定义正负样本的方式不同</strong>。<br>
PS 看每一行，分类时定义正负样本的方法对齐后，尽管回归方式不同，RetinaNet和FCOS(更进一步地，anchor-based和anchor-free)性能却基本相同。</p>
</li>
</ol>
<ul>
<li>
<p>提出改进<br>
FCOS提出的定义正负样本的方法优于之前基于IoU的方法，但这些方法都很依赖超参的设置。本文更进一步地，提出了无超参、鲁棒的、依赖数据特征的定义正负样本新方法ATSS(Adaptive Training Sample Selection)。<br>
<img src="https://FuNian788.github.io/post-images/object_detection/ATSS_5.png" alt="ATSS" loading="lazy"><br>
简要概括：对于GT G，在每一层找k个距其L2距离最近的anchor，组成候选positive anchor的集合P'。随后计算G和P'的IoU，统计其IoU的均值方差并修正，最后保留IoU大于修正值且中心在G内的所有positive anchor，将其组成正样本集合P；剩余的anchor作为负样本。<br>
补充优点：这种方法不会像IoU方法，对大物体表现出明显的青睐，在一定程度上保证了尺度的均衡性。<br>
实验表明，ATSS对于k的大小不敏感，对于anchor的scale和ratio不敏感，具有很好的鲁棒性。</p>
</li>
<li>
<p>实验结果<br>
<img src="https://FuNian788.github.io/post-images/object_detection/ATSS_6.png" alt="ATSS实验结果" loading="lazy"><br>
实验结果表现出了有趣的insight。第一行是原始RetinaNet(每个位置9个anchor)，后几行都是每个位置1个anchor。实验结果表明，对于传统的IoU筛选策略，在每个位置多设置一些anchor是有涨点的(37.0 -&gt; 38.4)。然而在优秀的筛选策略ATSS下，每个位置有一个anchor就够了，设置很多不同scale和ratio的anchor反倒没有什么用。</p>
</li>
<li>
<p>改进/Challenge/idea/Que</p>
</li>
</ul>
<hr>
<h3 id="5-2019-cvprhr-net-deep-high-resolution-representation-learning-for-visual-recognition">(5) <a href="https://arxiv.org/abs/1908.07919">(2019 CVPR)HR-Net: Deep High-Resolution Representation Learning for Visual Recognition</a></h3>
<ul>
<li>
<p>Motivation<br>
现有方法常先使用卷积层将输入编码至low-resolution representation，再从中恢复出high-resolution representation(全流程eg U-Net)。然而高分辨率信息对于position-sensitive的任务十分重要。</p>
</li>
<li>
<p>主要贡献<br>
HR-Net在全阶段都维持了高分辨率表示，从而获取更丰富的语义信息和更准确的空间位置。<br>
核心部件：<br>
(1) <strong>并行</strong>的高分辨率到低分辨率的卷积流(connect the high-to-low resolution convolution streams in parallel)<br>
(2) 重复融合多分辨率流的信息(repreatedly exchange the information across resolutions)</p>
</li>
<li>
<p>overall<br>
如下图所示，第一行一直保持高分辨率信息。四个阶段中，每阶段都在增加稍低分辨率的并行卷积流(一行行增加)，同时通过不同分辨率卷积流的信息交换实现多分辨率融合。<br>
<img src="https://FuNian788.github.io/post-images/object_detection/HR-Net_1.png" alt="overall" loading="lazy"><br>
如下图所示，融合不同分辨率特征的方法是有区别的：高分辨率到低分辨率使用n个stride为2的3*3卷积进行下采样；同分辨率直接相加；低分辨率到高分辨率使用n个(双线性差值+1*1卷积层)实现上采样和通道对齐。<br>
<img src="https://FuNian788.github.io/post-images/object_detection/HR-Net_2.png" alt="信息融合" loading="lazy"></p>
</li>
</ul>
<p>HRNetV1，HRNetV2和HRNetV2p的结构图如下所示。HRNetV1只输出高分辨率流，用于人体姿态估计；HRNetV2将低分辨率流上采样后将四个流的特征叠加起来，用于语义分割；HRNetV2p在HRNetV2的基础上多了下采样的过程，用于目标检测。<br>
<img src="https://FuNian788.github.io/post-images/object_detection/HR-Net_3.png" alt="backbone" loading="lazy"></p>
<ul>
<li>改进/Challenge/idea/Que<br>
Related Work中包含了很多文章，eg如何从低分辨率信息中学习，如何重建高分辨率信息，如何维持高分辨率信息，可以好好学习一下。</li>
</ul>
<hr>
<hr>
<h2 id="基础模型">基础模型</h2>
<h3 id="1-2017-cvprresnext-aggregated-residual-transformations-for-deep-neural-networks">(1) <a href="https://arxiv.org/abs/1611.05431">(2017 CVPR)ResNeXt: Aggregated Residual Transformations for Deep Neural Networks</a></h3>
<ul>
<li>Motivation<br>
VGG &amp; ResNet这类网络通过堆叠相同形状的block来构建深网络，且这种简洁的策略在不同数据集上表现出了优秀的鲁棒性；尽管Inception类网络可以在特定数据上以较低的运算开销展现卓越的性能，但其手工设计痕迹明显，任务迁移时需重新精调网络结构和超参数，成本较高。如何合并这两者？</li>
</ul>
<p>Inception结构通常遵循split-&gt;transform-&gt;merge范式，这是否是Inception成功的关键？(split: 使用许多1*1卷积将输入拆成多个低通道embedding；transform: 使用一些3*3和5*5的卷积来转换信息；merge: 使用通过concate将上述输出综合)</p>
<ul>
<li>
<p>主要贡献<br>
提出ResNeXt，既使用了repeat layer策略(from VGG &amp; ResNet)，又以一种简单、可扩展的方式利用split-transform-merge策略(from Inception)。</p>
</li>
<li>
<p>overall<br>
<img src="https://FuNian788.github.io/post-images/object_detection/ResNeXt_1.png" alt="backbone" loading="lazy"></p>
</li>
<li>
<p>实现细节</p>
</li>
<li>
<p>改进/Challenge/idea/Que</p>
</li>
</ul>
<hr>
<h3 id="n-2021-cvprpaper-name">(n) <a href="https://arxiv.org/abs/2103.09460">(2021 CVPR)PAPER NAME</a></h3>
<ul>
<li>
<p>Motivation</p>
</li>
<li>
<p>主要贡献</p>
</li>
<li>
<p>overall</p>
</li>
<li>
<p>实现细节</p>
</li>
<li>
<p>改进/Challenge/idea/Que</p>
</li>
</ul>
<hr>
<h2 id="基本常识">基本常识</h2>
<ol>
<li>论文中的<code>1x</code>代表以batch size为16，在COCO数据集上训练90k个iter，约为118287张图片上的12.17个epoch。2x等以此类推。</li>
<li>anchor：所有的anchor都参与classification分支的反向传播，此时的损失函数是在K+1类上的交叉熵损失函数，但只有positive anchor才参与regression分支的反向传播。positive的常用定义是IoU阈值，当然也存在一些根据近邻程度的匹配算法。</li>
<li>Anchor的设计存在很多定义方式，但都是在初始anchor(eg 16*16的正方形)上进行变换。<br>
在单feature map上(eg Faster RCNN)，可设定areas和aspect ratio；在多feature map上(eg FPN)，可在aspect ratio和areas之外设定scale。<br>
areas指面积，对应的是边长扩大比例(eg2的3、4、5次幂)，这样可得到原图像上对应的anchor大小eg 128*128，256*256，512*512。aspect ratio是在面积不变的情况下改变长宽比(eg 1:2，1:1，2:1)，以128*128为例可得到eg 184*96，128*128，96*184。这样就有三种面积，每种面积下三种比例，故RPN在每个点处生成9个候选框。<br>
scale指anchor的缩放比例，eg<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mn>2</mn><mn>0</mn></msup><mi mathvariant="normal">，</mi><msup><mn>2</mn><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mn>3</mn></mrow></msup><mo>=</mo><mn>1.26</mn><mi mathvariant="normal">，</mi><msup><mn>2</mn><mrow><mn>2</mn><mi mathvariant="normal">/</mi><mn>3</mn></mrow></msup><mo>=</mo><mn>1.59</mn></mrow><annotation encoding="application/x-tex">2^{0}，2^{1/3}=1.26，2^{2/3}=1.59</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span></span></span></span></span><span class="mord cjk_fallback">，</span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">/</span><span class="mord mtight">3</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">.</span><span class="mord">2</span><span class="mord">6</span><span class="mord cjk_fallback">，</span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">/</span><span class="mord mtight">3</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">.</span><span class="mord">5</span><span class="mord">9</span></span></span></span>。当然这也和FPN中有多种尺度的特征图输出有关。</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[MEGVII常用命令]]></title>
        <id>https://FuNian788.github.io/post/megvii-chang-yong-ming-ling/</id>
        <link href="https://FuNian788.github.io/post/megvii-chang-yong-ming-ling/">
        </link>
        <updated>2021-04-21T12:46:35.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%B5%81%E7%A8%8Bpipeline">目标检测流程pipeline</a>
<ul>
<li><a href="#%E5%88%9B%E5%BB%BAworkspace2">创建workspace2</a></li>
<li><a href="#%E8%84%9A%E6%9C%AC%E4%B8%80%E9%94%AE%E9%85%8D%E7%BD%AE%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83">脚本一键配置基础环境</a></li>
<li><a href="#%E5%AE%89%E8%A3%85anaconda">安装Anaconda</a></li>
<li><a href="#%E5%88%9B%E5%BB%BA%E5%B9%B6%E6%BF%80%E6%B4%BB%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83">创建并激活虚拟环境</a></li>
<li><a href="#%E5%AE%89%E8%A3%85rluanch%E5%92%8Crrun">安装rluanch和rrun</a></li>
<li><a href="#%E4%B8%8B%E8%BD%BDcvpack2">下载cvpack2</a>
<ul>
<li><a href="#%E5%BF%AB%E6%8D%B7%E6%96%B9%E6%B3%95">快捷方法</a></li>
<li><a href="#%E9%80%9A%E7%94%A8%E6%96%B9%E6%B3%95">通用方法</a></li>
</ul>
</li>
<li><a href="#%E9%85%8D%E7%BD%AEcvpack2">配置cvpack2</a></li>
<li><a href="#%E5%9C%A8cvpack2_playground%E4%B8%8B%E8%AE%AD%E7%BB%83">在cvpack2_playground下训练</a></li>
<li><a href="#inference">inference</a></li>
<li><a href="#vscode%E6%96%87%E4%BB%B6%E6%A0%8F%E8%B0%83%E6%95%B4">VScode文件栏调整</a></li>
<li><a href="#coco%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD">coco数据集下载</a></li>
</ul>
</li>
<li><a href="#oss%E5%91%BD%E4%BB%A4">oss命令</a>
<ul>
<li><a href="#rlaunch%E5%91%BD%E4%BB%A4">rlaunch命令</a></li>
<li><a href="#ssh-permission-denied">SSH permission denied</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</p>
<h2 id="目标检测流程pipeline">目标检测流程pipeline</h2>
<h3 id="创建workspace2">创建workspace2</h3>
<ul>
<li>在<a href="https://www.brainpp.cn/hh-b/console/job?type=ws2">计算平台</a>创建一个workspace2。</li>
<li>创建好后复制内网ssh，eg<code>ssh -CAXY minguk.lizexian.ws2@hh-b-internal.brainpp.cn</code>，使用VScode的remote-SSH插件打开。如有问题可直接配置<code>/Users/lizexian/.ssh/config</code>如下：</li>
</ul>
<pre><code class="language-Shell">Host hh-b-internal.brainpp.cn
  HostName hh-b-internal.brainpp.cn
  Compression yes
  ForwardAgent yes
  ForwardX11 yes
  ForwardX11Trusted yes
  User minguk.lizexian.ws2
</code></pre>
<h3 id="脚本一键配置基础环境">脚本一键配置基础环境</h3>
<ul>
<li>下载ntools</li>
</ul>
<pre><code class="language-Shell">git clone git@git-core.megvii-inc.com:research_model/ntools2.git
</code></pre>
<ul>
<li>一键配置</li>
</ul>
<pre><code class="language-Shell">cd ntools2/setup
sudo chown -R $USER ~/.local/    
./setup_ws2.sh &amp;&amp; source ~/.bashrc
cd ..
./configure
./install.sh
</code></pre>
<ul>
<li>配置并测试oss</li>
</ul>
<pre><code class="language-Shell">aws configure
# Get below from 'https://www.brainpp.cn/account/security'
# input E7dv5T2Qm9V1ljMemHY9
# input L4p-snrN5uGMKa86z0QAV4grERktTeef
# input NONE
# input NONE
oss ls
</code></pre>
<h3 id="安装anaconda">安装Anaconda</h3>
<ul>
<li>下载anaconda包</li>
</ul>
<pre><code class="language-shell">cd /home/lizexian
oss cp s3://lizexian/Anaconda3-5.3.1-Linux-x86_64.sh .
</code></pre>
<ul>
<li>安装anaconda</li>
</ul>
<pre><code class="language-Shell">bash Anaconda3-5.3.1-Linux-x86_64.sh
</code></pre>
<p>安装过程一路回车即可，默认刷新bash，不安装vscode</p>
<ul>
<li>刷新bashrc</li>
</ul>
<pre><code class="language-Shell">source ~/.bashrc
</code></pre>
<p>如果没有修改默认bash，可通过如下方法激活conda</p>
<blockquote>
<p><code>vim ~/.bashrc</code><br>
<code>shift + g</code> jump to the last line<br>
<code>export PATH=&quot;/home/lizexian/anaconda3/bin:$PATH&quot;</code><br>
<code>source ~/.bashrc</code></p>
</blockquote>
<p>如果无法使用conda activate激活，可使用如下</p>
<blockquote>
<p><code>source activate cvpack</code></p>
</blockquote>
<h3 id="创建并激活虚拟环境">创建并激活虚拟环境</h3>
<pre><code class="language-shell">conda create -n cvpack python=3.7
conda activate cvpack
</code></pre>
<blockquote>
<p>如在wuhu-a集群遇到HTTPERROR时，可修改conda源：修改~/.condarc如下</p>
</blockquote>
<pre><code class="language-Shell">channels:
  - https://mirrors.ustc.edu.cn/anaconda/pkgs/main/
  - https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/
  - https://mirrors.ustc.edu.cn/anaconda/pkgs/free/
show_channel_urls: true
</code></pre>
<h3 id="安装rluanch和rrun">安装rluanch和rrun</h3>
<pre><code class="language-Shell">sudo apt update &amp;&amp; sudo apt install -y brainpp
pip3 install --user rrun
</code></pre>
<h3 id="下载cvpack2">下载cvpack2</h3>
<h4 id="快捷方法">快捷方法</h4>
<ul>
<li>下载预打包的YOLOF_cvpack2</li>
</ul>
<pre><code class="language-Shell">oss cp s3://lizexian/yolof0511.zip .
unzip -qq yolof.zip
# zip -q -r yolof.zip cvpack2/
# oss cp yolof0511.zip s3://lizexian
</code></pre>
<h4 id="通用方法">通用方法</h4>
<ul>
<li>下载cvpack2</li>
</ul>
<pre><code class="language-Shell"># 1. Download without cvpack2_playground
# git clone git@git-core.megvii-inc.com:base-detection/cvpack2.git
# 2. Download with cvpack2_playground
git clone --recursive git@git-core.megvii-inc.com:zhubenjin/cvpack2.git
</code></pre>
<ul>
<li>修改requirement.txt</li>
</ul>
<pre><code class="language-Shell">torch==1.6.0 # torch==1.7.1
torchvision==0.7.0 # torchvision==0.8.2
</code></pre>
<ul>
<li>根据YOLOF修改cvpack2<br>
详见YOLOF的README.txt文档</li>
</ul>
<h3 id="配置cvpack2">配置cvpack2</h3>
<ul>
<li>安装依赖项并下载COCO数据集</li>
</ul>
<pre><code class="language-Shell">pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'
cd cvpack2
bash tools/setup/install.sh all hhb
# 'all' option will download COCO dataset to '/data/Datasets/COCO' while 'lib' will not.
# datasets/cvpack2_prepare coco
</code></pre>
<p>如果占卡的时候遇到问题，可直接使用如下语句进行手动编译：</p>
<blockquote>
<p><code>rlaunch --cpu=8 --gpu=2 --memory=10240 -- bash</code><br>
<code>python setup.py build develop --user</code></p>
</blockquote>
<p>再手动测试是否安装成功</p>
<blockquote>
<p><code>conda activate cvpack</code><br>
<code>python</code><br>
<code>import cvpack2</code></p>
</blockquote>
<p>单独下载COCO数据集的方法</p>
<blockquote>
<p><code>oss sync --no-progress s3://generalDetection/mscoco_raw_data/coco2017 /data/Datasets/COCO</code><br>
<code># val2017.zip 815585330</code><br>
<code># train2017.zip 19336861798</code><br>
<code>unzip -qq annotations_trainval2017.zip</code><br>
<code>unzip -qq val2017.zip # val2017: 163840</code><br>
<code>unzip -qq train2017.zip # train2017: 3846144</code></p>
</blockquote>
<h3 id="在cvpack2_playground下训练">在cvpack2_playground下训练</h3>
<ul>
<li>建立数据集的软连接</li>
</ul>
<pre><code class="language-Shell"># build. `ln -s source_address target_address`
ln -s /data/Datasets/COCO  /home/lizexian/cvpack2/datasets/coco

# delete. `rm -rf address`
# NOTE not `rm -rf ./test_cjk_in/` which will delete files instead of link.
rm -rf  ./test_chk_ln 
</code></pre>
<ul>
<li>占卡及训练</li>
</ul>
<pre><code class="language-Shell">tmux
rlaunch --cpu=8 --gpu=8 --memory=200000 -- bash
cd /home/lizexian/cvpack2/cvpack2_playground/examples/detection/coco/yolof/yolofv1.res50.1x
conda activate cvpack
cvpack2_train --num-gpus 8
</code></pre>
<blockquote>
<p>YOLOF至少需要4张卡</p>
</blockquote>
<h3 id="inference">inference</h3>
<p>模型会自动进行inference，结果保存在<code>/data/Outputs/model_logs/cvpack2_playground/examples/detection/coco/yolof/yolofv1.res50.1x/inference/coco_instances_results.json</code>路径下。对应地，该路径下的log、pth等文件会自动上传到<code>s3://cvpack2dumps/lizexian/playground/examples/detection/coco/yolof/yolofv1.res50.1x/</code>路径下。</p>
<h3 id="vscode文件栏调整">VScode文件栏调整</h3>
<ul>
<li>Command + , 呼叫设置栏</li>
<li>在Files: exclude中添加忽略模式</li>
</ul>
<pre><code class="language-json">&quot;files.exclude&quot;: {
  &quot;**/.aws&quot;: true,
  &quot;**/.bash_history&quot;: true,
  &quot;**/.bash_logout&quot;: true,
  &quot;**/.bashrc&quot;: true,
  &quot;**/.bashrc-anaconda3.bak&quot;: true,
  &quot;**/.cache&quot;: true,
  &quot;**/.conda&quot;: true,
  &quot;**/.config&quot;: true,
  &quot;**/.gnupg&quot;: true,
  &quot;**/.local&quot;: true,
  &quot;**/.nv&quot;: true,
  &quot;**/.profile&quot;: true,
  &quot;**/.python_history&quot;: true,
  &quot;**/.rlaunch&quot;: true,
  &quot;**/.ssh&quot;: true,
  &quot;**/.sudo_as_admin_successful&quot;: true,
  &quot;**/.tmux.conf&quot;: true,
  &quot;**/.vim&quot;: true,
  &quot;**/.vimrc&quot;: true,
  &quot;**/.wgetrc&quot;: true,
  &quot;**/.Xauthority&quot;: true
}
</code></pre>
<h3 id="coco数据集下载">coco数据集下载</h3>
<p>optionB+d退出<br>
tmux attach<br>
optionB+N切换</p>
<p>s3://cvpack2dump/root/<br>
/data/outputs/、</p>
<p>base config line 46</p>
<p>cvpack2_test --num-gpus 8 <br>
MODEL.WEIGHTS /path/to/your/checkpoint.pth \ # if necessary<br>
OUTPUT_DIR /path/to/save_dir # don't need to specify this in defalut</p>
<h2 id="oss命令">oss命令</h2>
<p>自有路径：<code>lizexian</code> or <code>s3://lizexian</code><br>
oss的常用指令包括：ls、cp、mv、rm、sync。可使用 --exclude 和 --include 选项指定规则来筛选要操作的文件或者目录。</p>
<pre><code class="language-Shell">oss ls lizexian/file
oss cp 1.py s3://lizexian
oss cp yolof s3://lizexian --recursive
oss mv 1.py s3://lizexian
oss rm s3://lizexian/*.py
</code></pre>
<h3 id="rlaunch命令">rlaunch命令</h3>
<p>使用rluanch命令动态申请资源并运行程序</p>
<pre><code class="language-Shell">rlaunch --cpu=4 --gpu=4 --memory=30000 -- python3 train.py
rlaunch --cpu=4 --gpu=4 --memory=30000 -- bash
rlaunch -P1 --cpu=8 --gpu=8 --memory=200000 -- cvpack2_train --num-gpus 8   # can be killed
rlaunch --predict-only
</code></pre>
<h3 id="ssh-permission-denied">SSH permission denied</h3>
<pre><code class="language-Shell">ssh-add ~/.ssh/id_rsa_megvii
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[论文阅读 - SALAD: Accurate Temporal Action Proposal Generation with Relation-Aware Pyramid Network]]></title>
        <id>https://FuNian788.github.io/post/salad/</id>
        <link href="https://FuNian788.github.io/post/salad/">
        </link>
        <updated>2021-02-05T06:45:20.000Z</updated>
        <content type="html"><![CDATA[<h3 id="针对痛点">针对痛点</h3>
<ol>
<li>proposal的边界回归都是直接回归一个值，并没有人探索过此回归过程的置信度得分<br>
说白了就是动作边界的回归很难，一些简单的任务，eg目标检测的边界回归就不需要置信度得分，直接用回归值获取bbox的结果就已经很棒了。本文考虑的就是使用回归过程的置信度来优化回归流程。</li>
</ol>
<h3 id="主要贡献">主要贡献</h3>
<ol>
<li>提出联合训练方法：在执行回归任务的同时评估该过程的置信度<br>
自评估模块(self-assessment module)通过注意力线索和多任务正则化(multi-task regularization)方法，可以在训练过程中有效删除低分片段，提升特征质量。<br>
注意力线索：使用置信度修正训练过程，即删除得分较低的侯选框<br>
多任务正则化：指加了评估回归置信度的分支</li>
</ol>
<h3 id="总体思路">总体思路</h3>
<figure data-type="image" tabindex="1"><img src="https://FuNian788.github.io/post-images/TAD/SALAD_2.png" alt="SALAD网络结构图" loading="lazy"></figure>
<ul>
<li>使用I3D提取视频特征；</li>
<li>对于每个时刻t，由双向GRU输出两段分别表征此前所有时刻/此后所有时刻信息的特征；</li>
<li>通过时刻t处得到的两段特征，使用三个并联的全连接模块分别输出包含时刻t的回归后proposal、该回归过程的置信度得分和proposal的动作类别。<br>
注：回归proposal为anchor-free思路，近似于FCOS，直接回归某一时刻点对应的开始、结束时刻。</li>
</ul>
<h3 id="naive自评估边界回归naive-regeression-self-assessment">Naive自评估边界回归(Naive regeression self-assessment)</h3>
<p>不像多分类任务时，模型输出样本属于每一类别的置信度；回归任务中模型只输出回归值，而不评估该过程的置信度。<br>
评估每个侯选框在边界回归过程的得分的Naive方法：使用一个双头网络，一个头执行回归任务，一个头评估我们能否信任这个回归值。这时，这种置信度评估就是一个二分类任务。<br>
对于输入时刻x，其对应的Ground Truth为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">{z}(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>，网络回归结果为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>z</mi><mo>^</mo></mover><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\hat{z}(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;">^</span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>，自评估得到的置信度为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\hat{p}(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">p</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>，设k为针对回归结果的二值化阈值，则我们可以得到如下损失函数：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mo>=</mo><mi mathvariant="normal">∥</mi><mi>z</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mover accent="true"><mi>z</mi><mo>^</mo></mover><mo>(</mo><mi>x</mi><mo>)</mo><msubsup><mi mathvariant="normal">∥</mi><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>α</mi><mrow><mo fence="true">[</mo><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>y</mi><mo>(</mo><mi>x</mi><mo>)</mo><mi>log</mi><mo>⁡</mo><mo>(</mo><mover accent="true"><mi>p</mi><mo>^</mo></mover><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>+</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mi>log</mi><mo>⁡</mo><mo>(</mo><mn>1</mn><mo>−</mo><mover accent="true"><mi>p</mi><mo>^</mo></mover><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">l=\|z(x)-\hat{z}(x)\|_{2}^{2}+\alpha\left[\begin{array}{c} y(x) \log (\hat{p}(x))+ \\ (1-y(x)) \log (1-\hat{p}(x)) \end{array}\right]
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∥</span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;">^</span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">p</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mclose">)</span><span class="mord">+</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">p</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">]</span></span></span></span></span></span></span></p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mn>1</mn><mtext> </mtext><mi>i</mi><mi>f</mi><mtext> </mtext><mi mathvariant="normal">∥</mi><mi>z</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mover accent="true"><mi>z</mi><mo>^</mo></mover><mo>(</mo><mi>x</mi><mo>)</mo><msubsup><mi mathvariant="normal">∥</mi><mn>2</mn><mn>2</mn></msubsup><mo>&lt;</mo><mi>κ</mi><mtext> </mtext><mi>e</mi><mi>l</mi><mi>s</mi><mi>e</mi><mtext> </mtext><mn>0</mn></mrow><annotation encoding="application/x-tex">y(x) = 1 \ if \ \|z(x)-\hat{z}(x)\|_{2}^{2}&lt;\kappa \ else \ 0  
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mspace"> </span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mspace"> </span><span class="mord">∥</span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;">^</span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">κ</span><span class="mspace"> </span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">s</span><span class="mord mathdefault">e</span><span class="mspace"> </span><span class="mord">0</span></span></span></span></span></p>
<h3 id="自评估动作检测action-detection-self-assessment">自评估动作检测(Action detection self-assessment)</h3>
<p>论文中自评估动作检测使用如下方法：<br>
构建两个矩阵：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><msub><mi>α</mi><mrow><mi>t</mi><mo separator="true">,</mo><mi>n</mi></mrow></msub><msub><mo>)</mo><mrow><mi>t</mi><mo separator="true">,</mo><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">(\alpha_{t,n})_{t,n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.28055599999999997em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.28055599999999997em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>矩阵是0-1矩阵，表征N个GT在时长为T的视频上具体的持续时间；<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><msub><mi>y</mi><mi>t</mi></msub><msub><mo>)</mo><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">(y_t)_{t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>矩阵是0-1矩阵，表征时长为T的视频中从哪些时刻回归得到的侯选框被选中了。<br>
这两个矩阵的构建思路如下图所示：<br>
<img src="https://FuNian788.github.io/post-images/TAD/SALAD_4.png" alt="矩阵构建思路" loading="lazy"><br>
先根据置信度对所有时刻进行排序，然后遍历所有时刻，再遍历所有GT，为每个GT找且仅找一个tIoU大于阈值的回归侯选框。<br>
对应地，可以基于此构建损失函数：<br>
<img src="https://FuNian788.github.io/post-images/TAD/SALAD_5.png" alt="损失函数" loading="lazy"><br>
此时的损失函数和Naive差不多，主要是把边界点的L2 loss改成了tIoU loss。<br>
自评估动作检测的训练流程如下所示：<br>
<img src="https://FuNian788.github.io/post-images/TAD/SALAD_3.png" alt="自评估动作检测" loading="lazy"><br>
这是一个动态过程，开始时，T时刻全部参与训练；逐渐地，一些潜力不足的时刻由于得分过低被删去。如下图所示，黑色箭头代表五个时刻点生成的五个侯选框，侯选框蓝色的深浅代表网络输出的得分，与任意GT无tIoU的proposal直接被删除(如左图灰色X)，tIoU小于阈值的proposal也逐渐被舍弃(如左图红色X)，一些较好的proposal的边界在逐渐被矫正；右图表示在每次仅优化一个得分最高的侯选框过程中，一些得分在逐渐降低，只有一个的得分在逐渐升高。<br>
这种不断去除较差样本的思路近似于强化学习，即不是所有样本都适合用于全流程训练。</p>
<p>最终的损失函数包括对所有时刻做帧级K+1类动作分类的交叉熵损失函数。</p>
<h3 id="讲故事">讲故事</h3>
<p>故事：置信度可以视为理解算法本身的一种方式；评估自己做的好不好很重要；心理学说，如果可以很好地评估他人的能力有助于增强自身水平。</p>
<p>联合训练和自评估就那么回事，就是多加了一个头，得到当前回归proposal的置信度，但这个置信度很好地被利用到了损失函数构建和训练过程中，这一点论文反倒没有强调。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[常用损失函数合集(Pytorch)]]></title>
        <id>https://FuNian788.github.io/post/Pytorch-loss/</id>
        <link href="https://FuNian788.github.io/post/Pytorch-loss/">
        </link>
        <updated>2020-12-11T07:41:26.000Z</updated>
        <content type="html"><![CDATA[<p>训model嘛，损失函数总是要得。这里简单归纳一下常见的损失函数及其在Pytorch中的使用方法，以便查阅👨‍💻。<br>
<ul class="markdownIt-TOC">
<li>
<ul>
<li>
<ul>
<li><a href="#1-%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0">1. 交叉熵损失函数</a>
<ul>
<li><a href="#nnlogsoftmax">nn.LogSoftmax()</a></li>
<li><a href="#nnnllloss">nn.NLLLoss()</a></li>
<li><a href="#nncrossentropyloss">nn.CrossEntropyLoss()</a></li>
<li><a href="#%E7%A4%BA%E4%BE%8B">示例</a></li>
</ul>
</li>
<li><a href="#2-%E4%BA%8C%E5%88%86%E7%B1%BB%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0">2. 二分类交叉熵损失函数</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</p>
<h3 id="1-交叉熵损失函数">1. 交叉熵损失函数</h3>
<p>交叉熵常用于多分类任务，在Pytorch中使用torch.nn.CrossEntropyLoss()函数实现。底层地，该函数是由<code>nn.LogSoftmax()</code>函数和<code>nn.NLLLoss()</code>函数结合得到的。</p>
<h4 id="nnlogsoftmax">nn.LogSoftmax()</h4>
<p>很简单的，<code>nn.LogSoftmax()</code>就是在Softmax()的基础上取对数(ln，以e为底)。</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">LogSoftmax</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence="true">)</mo></mrow><mo>=</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence="true">)</mo></mrow></mrow><mrow><munder><mo>∑</mo><mi>j</mi></munder><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo fence="true">)</mo></mrow></mrow></mfrac><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{LogSoftmax}\left(x_{i}\right)=\log \left(\frac{\exp \left(x_{i}\right)}{\sum_{j} \exp \left(x_{j}\right)}\right)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop"><span class="mord mathrm">L</span><span class="mord mathrm">o</span><span class="mord mathrm" style="margin-right:0.01389em;">g</span><span class="mord mathrm">S</span><span class="mord mathrm">o</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span><span class="mord mathrm">t</span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0000299999999998em;vertical-align:-1.25003em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">exp</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop">exp</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.1218180000000002em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">)</span></span></span></span></span></span></span></p>
<h4 id="nnnllloss">nn.NLLLoss()</h4>
<p><code>nn.NLLLoss()</code>即负对数似然函数(negative log likelihood loss)，常用于处理K分类问题。该损失函数的输入是对数概率向量和类别标签，适合最后一层是<code>nn.LogSoftmax()</code>的网络。<br>
参考下述代码中loss1及loss2的计算方法去理解：<code>nn.NLLLoss()</code>就是将<code>nn.LogSoftmax()</code>输出中与label对应的值拿出来，去掉负号，再取均值。<br>
以3张图片的5分类任务为例，我们假设label为0,2,4，我们将第一张图片网络输出五个值中的第一个值(index=0)、第二张图片输出的第三个值(index=2)、第三张图片输出的第五个值(index=4)拿出来，将这个三个数取相反数，求平均值，即为负对数似然函数的输出。</p>
<h4 id="nncrossentropyloss">nn.CrossEntropyLoss()</h4>
<h4 id="示例">示例</h4>
<pre><code class="language-Python">import torch
import torch.nn as nn

inputs = torch.randn(3, 5)
label = torch.tensor([0, 2, 4])
print(inputs)
'''
tensor([[-4.0413e-01, -2.3209e+00,  7.4667e-01, -1.5884e-01, -9.4237e-01],
        [ 9.7085e-02,  6.3694e-02, -1.7933e-01,  2.6058e-01, -1.9870e-01],
        [ 1.0408e-01,  5.2098e-05, -9.9802e-01, -2.0989e-01,  1.1665e+00]])
'''

log_softmax = nn.LogSoftmax(dim=1)
net = log_softmax(inputs)
print(net)
'''
tensor([[-1.8196, -3.7364, -0.6688, -1.5743, -2.3579],
        [-1.5363, -1.5697, -1.8127, -1.3728, -1.8321],
        [-1.7677, -1.8717, -2.8698, -2.0817, -0.7052]])
'''

# NLLLoss after LogSoftmax.
nll = nn.NLLLoss()
loss1 = nll(net, label)
print('NLLLoss value after LogSoftmax layer is {}.'.format(loss1))  
# 1.4459

# Simulink the process of NLLLoss.
loss2 = 0.
for i in range(len(label)):
    loss2 += net[i][label[i]]
loss2 = (-1.0) * loss2 / len(label)
print(&quot;Simulink the process of NLLLoss to get loss: {}.&quot;.format(loss2))  
# 1.4459

# CrossEntropyLoss: combination of Softmax, log and NLLLoss.
ce = nn.CrossEntropyLoss()
loss3 = ce(inputs, label)
print(&quot;Loss from CrossEntropyLoss is {}.&quot;.format(loss3))
# 1.4459
</code></pre>
<h3 id="2-二分类交叉熵损失函数">2. 二分类交叉熵损失函数</h3>
]]></content>
    </entry>
</feed>